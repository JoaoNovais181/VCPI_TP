{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import vcpi_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanceamento do dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olhando para o output da célula abaixo podemos verificar que existem classes muito subpopuladas em relação a outras, pelo que achamos que seria uma mais valia balancear o dataset, adicionando entradas que seriam transformações de outras entradas do dataset.\n",
    "\n",
    "No final, o dataset balanceado tem 2000 entradas para todas as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 1830, 1: 1801, 13: 1739, 12: 1712, 38: 1639, 14: 1635, 5: 1631, 34: 1630, 41: 1622, 9: 1622, 7: 1617, 31: 1616, 42: 1616, 15: 1616, 26: 1606, 22: 1605, 17: 1605, 3: 1605, 18: 1602, 6: 1601, 33: 1601, 8: 1599, 20: 1599, 19: 1598, 36: 1597, 40: 1597, 23: 1597, 29: 1594, 4: 1589, 24: 1589, 16: 1589, 11: 1587, 39: 1586, 27: 1585, 37: 1584, 21: 1583, 28: 1582, 35: 1580, 25: 1579, 0: 1577, 10: 1571, 32: 1568, 30: 1567})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_list = []\n",
    "for images, targets in train_loader:\n",
    "    target_list.extend(targets)\n",
    "\n",
    "vcpi_util.show_histogram(target_list, train_set.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PER_CLASS = 2000\n",
    "data_path = 'dataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from PIL import Image, ImageEnhance\n",
    "# import random\n",
    "# import os\n",
    "\n",
    "# try:\n",
    "#     os.mkdir(f\"{data_path}/train_balanced\")\n",
    "# except:\n",
    "#     print(\"Ja tinha folder\")\n",
    "# classes = os.listdir(f'{data_path}/train')\n",
    "\n",
    "# list_img = []\n",
    "# for cla in classes:\n",
    "#     try:\n",
    "#         os.mkdir(f\"{data_path}/train_balanced/{cla}\")\n",
    "#     except:\n",
    "#         print(\"Ja tinha folder\")\n",
    "    \n",
    "#     list_img = [file for file in os.listdir(f'{data_path}/train/{cla}') if \".csv\" not in file]\n",
    "#     random.shuffle(list_img)\n",
    "    \n",
    "#     for i in range(len(list_img)):\n",
    "#         filename = f\"{data_path}/train/{cla}/{list_img[i]}\"\n",
    "#         im = Image.open(filename)\n",
    "        \n",
    "#         im.save(f\"{data_path}/train_balanced/{cla}/{list_img[i]}\")\n",
    "    \n",
    "#     for k in range(len(list_img), IMAGES_PER_CLASS):\n",
    "\n",
    "#         filename = f'{data_path}/train/{cla}/{list_img[(k - len(list_img)) % len(list_img)]}'\n",
    "#         im = Image.open(filename)\n",
    "\n",
    "#         r = random.uniform(-10.0,10.0)\n",
    "#         im = im.rotate(r)\n",
    "#         r1 = random.uniform(-3.0,3.0)\n",
    "#         r2 = random.uniform(-3.0,3.0)\n",
    "\n",
    "#         im = im.transform(im.size, Image.Transform.AFFINE, (1, 0, r1, 0, 1, r2))\n",
    "\n",
    "#         r = random.uniform(1.0, 1.3)\n",
    "#         im = ImageEnhance.Sharpness(im)\n",
    "#         im = im.enhance(r)\n",
    "\n",
    "#         r = random.uniform(1.0, 1.3)\n",
    "#         im = ImageEnhance.Contrast(im)\n",
    "#         im = im.enhance(r)\n",
    "        \n",
    "#         im = im.resize((32,32))\n",
    "\n",
    "#         im.save(f'{data_path}/train_balanced/{cla}/_{k}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições Relevantes\n",
    "\n",
    "Abaixo temos a definição da função de treino para um modelo, que recebe também uma transformação para aplicar dinamicamente às imagens enquanto treina, fazendo assim aumentação do dataset de treino. Temos ainda a definição da função para avaliar um modelo, a definição do modelo em si, do early stopper utilizado para parar mais cedo o treino, e ainda uma função que constrói a matrix de confusão dos resultados do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_III(model, train_loader, val_loader, epochs, loss_fn, optimizer, scheduler, early_stopper, transform: transforms.Compose = None, save_prefix = 'model'):\n",
    "\n",
    "    history = {}\n",
    "    history['accuracy'] = []\n",
    "    history['val_acc'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['loss'] = []\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        model.train()\n",
    "        start_time = time.time() \n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(train_loader, 0):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # inputs, targets = transform(inputs, targets)\n",
    "            if transform:\n",
    "                inputs = transform(inputs)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            # targets = targets.squeeze()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # predicted = outputs\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss\n",
    "            correct += (predicted == targets).sum()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            v_correct = 0\n",
    "            val_loss = 0.0\n",
    "            for i,t in val_loader:\n",
    "                \n",
    "                i = i.to(device)\n",
    "                t = t.to(device)\n",
    "                if transform:\n",
    "                    i = transform(i)\n",
    "                # i,t= transform(i,t)\n",
    "                o = model(i)\n",
    "                # t = t.squeeze()\n",
    "                _,p = torch.max(o,1)\n",
    "                # p = o\n",
    "                #with torch.no_grad():\n",
    "                val_loss += loss_fn(o, t)\n",
    "\n",
    "                v_correct += (p == t).sum()\n",
    "\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if old_lr != new_lr:\n",
    "            print('==> Learning rate updated: ', old_lr, ' -> ', new_lr)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        accuracy = 100 * correct / len(train_loader.dataset)\n",
    "        v_accuracy = 100 * v_correct / len(val_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        stop_time = time.time()\n",
    "        print(f'Epoch: {epoch:03d}; Loss: {epoch_loss:0.6f}; Accuracy: {accuracy:0.4f}; Val Loss: {val_loss:0.6f}; Val Acc: {v_accuracy:0.4f}; Elapsed time: {(stop_time - start_time):0.4f}')\n",
    "        history['accuracy'].append(accuracy.cpu().numpy())\n",
    "        history['val_acc'].append(v_accuracy.cpu().numpy())\n",
    "        history['val_loss'].append(val_loss.cpu().detach().numpy())\n",
    "        history['loss'].append(epoch_loss.cpu().detach().numpy())\n",
    " \n",
    "        ###### Saving ######\n",
    "        if val_loss < best_val_loss:\n",
    "           \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model':model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "                },\n",
    "                f'{save_prefix}_best.pt')\n",
    "\n",
    "        if early_stopper(val_loss):\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "    return(history)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "\n",
    "    # sets the model in evaluation mode.\n",
    "    # although our model does not have layers which behave differently during training and evaluation\n",
    "    # this is a good practice as the models architecture may change in the future\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "         \n",
    "        # forward pass, compute the output of the model for the current batch\n",
    "        outputs = model(images.to(device))\n",
    "\n",
    "        # \"max\" returns a namedtuple (values, indices) where values is the maximum \n",
    "        # value of each row of the input tensor in the given dimension dim; \n",
    "        # indices is the index location of each maximum value found (argmax).\n",
    "        # the argmax effectively provides the predicted class number        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        correct += (preds.cpu() == targets).sum()\n",
    "\n",
    "    return (correct / len(data_loader.dataset)).item()\n",
    "\n",
    "\n",
    "class Conv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, 3)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, 3)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(32)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(32, 48, 3)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(48)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv4 = torch.nn.Conv2d(48, 48, 3)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(48)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(1200, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):    \n",
    "        \n",
    "        # input = (bs, 3, 32, 32)\n",
    "        x = self.conv1(x) # -> (bs, 16, 30, 30)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x) # -> (bs, 32, 28, 28)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool1(x) # -> (bs, 32, 14, 14)\n",
    "        \n",
    "        x = self.conv3(x) # -> (bs, 48, 12, 12)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x) # -> (bs, 48, 10, 10)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool2(x) # -> (bs, 48, 5, 5)\n",
    "        \n",
    "        x = torch.flatten(x,1) # -> (bs, 48 * 5 * 5 = 1200)\n",
    "        x = self.fc1(x)        # -> (bs, num_classes)\n",
    "\n",
    "        return(x)\n",
    "\n",
    "\n",
    "\n",
    "class Early_Stopping():\n",
    "\n",
    "    def __init__(self, patience = 3, min_delta = 0.00001):\n",
    "\n",
    "        self.patience = patience \n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        self.min_delta\n",
    "        self.min_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        # improvement\n",
    "        if val_loss + self.min_delta < self.min_val_loss:\n",
    "            self.min_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "        # no improvement            \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def build_confusion_matrix(model, dataset):\n",
    "\n",
    "    preds = []\n",
    "    ground_truth = []\n",
    "\n",
    "    for images, targets in dataset:\n",
    "\n",
    "        predictions = model(images.to(device))\n",
    "        preds_sparse = [np.argmax(x) for x in predictions.cpu().detach().numpy()]\n",
    "        preds.extend(preds_sparse)\n",
    "        ground_truth.extend(targets.numpy())\n",
    "\n",
    "    vcpi_util.show_confusion_matrix(ground_truth, preds, len(test_set.classes))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decidimos utilizar 32x32 para o tamanho das imagens dentro do modelo, com um _batch size_ de 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "PATH_TRAINING_SET = 'dataset/train_balanced'\n",
    "PATH_TEST_SET = 'dataset/test'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPOCHS = 30\n",
    "img_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição de uma transformação que foi utilizada no treino de um modelo, que apenas roda a imagem e apaga parte da mesma, e uma transformação que apenas redimensiona a imagem e a coloca no tipo float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomRotation(50),\n",
    "    v2.Resize((img_size, img_size)), \n",
    "    v2.RandomErasing(0.5, (0.1,0.1)),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "transformNormal = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((img_size, img_size)), \n",
    "    v2.ToDtype(torch.float32)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dos datasets de treino e de teste, utilizando 80% do dataset de treino para efetivamente treinar o modelo e 20% para validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set: torchvision.datasets.ImageFolder = torchvision.datasets.ImageFolder(root=PATH_TRAINING_SET, transform = transformNormal)\n",
    "\n",
    "train_sub_set, val_sub_set = torch.utils.data.random_split(train_set, [0.8, 0.2])\n",
    "\n",
    "train_loader: torch.utils.data.DataLoader = torch.utils.data.DataLoader(train_sub_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader: torch.utils.data.DataLoader = torch.utils.data.DataLoader(val_sub_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_set: torchvision.datasets.ImageFolder = torchvision.datasets.ImageFolder(root=PATH_TEST_SET, transform = transformNormal)\n",
    "test_loader: torch.utils.data.DataLoader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (conv4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu4): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1200, out_features=43, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Conv                                     [32, 43]                  --\n",
       "├─Conv2d: 1-1                            [32, 16, 30, 30]          448\n",
       "├─BatchNorm2d: 1-2                       [32, 16, 30, 30]          32\n",
       "├─ReLU: 1-3                              [32, 16, 30, 30]          --\n",
       "├─Conv2d: 1-4                            [32, 32, 28, 28]          4,640\n",
       "├─BatchNorm2d: 1-5                       [32, 32, 28, 28]          64\n",
       "├─ReLU: 1-6                              [32, 32, 28, 28]          --\n",
       "├─MaxPool2d: 1-7                         [32, 32, 14, 14]          --\n",
       "├─Conv2d: 1-8                            [32, 48, 12, 12]          13,872\n",
       "├─BatchNorm2d: 1-9                       [32, 48, 12, 12]          96\n",
       "├─ReLU: 1-10                             [32, 48, 12, 12]          --\n",
       "├─Conv2d: 1-11                           [32, 48, 10, 10]          20,784\n",
       "├─BatchNorm2d: 1-12                      [32, 48, 10, 10]          96\n",
       "├─ReLU: 1-13                             [32, 48, 10, 10]          --\n",
       "├─MaxPool2d: 1-14                        [32, 48, 5, 5]            --\n",
       "├─Linear: 1-15                           [32, 43]                  51,643\n",
       "==========================================================================================\n",
       "Total params: 91,675\n",
       "Trainable params: 91,675\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 261.40\n",
       "==========================================================================================\n",
       "Input size (MB): 0.39\n",
       "Forward/backward pass size (MB): 26.23\n",
       "Params size (MB): 0.37\n",
       "Estimated Total Size (MB): 26.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conv(len(train_set.classes))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "torchinfo.summary(model, input_size=(BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição do optimizer, scheduler, loss_fn e early_stop utilizados para treinar o modelo.\n",
    "\n",
    "Temos ainda a definição de algumas transformações, 5 para ser exato.\n",
    "\n",
    "* transform - Aplicação de uma rotação aleatória entre 0 e 50 graus, redimensionamento e apagar parte da imagem\n",
    "* transform2 - Redimensionamento da imagem e permutação dos canais da cor, afetando apenas a cor das imagens\n",
    "* transform3 - Espelhar a imagem horizontalmente (com 50% de chance), espelhar a imagem verticalmente (com 50% de chance), mudar a perspetiva da imagem e aplicar uma transformação \"affine\"\n",
    "* transform4 - chance aleatória entre um cutmix ou um mixup de imagens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor = 0.1, patience=3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "early_stop = Early_Stopping(9)\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((img_size, img_size)), \n",
    "    v2.RandomChannelPermutation(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "transform3 = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    v2.RandomVerticalFlip(0.5),\n",
    "    v2.RandomPerspective(0.5),\n",
    "    v2.RandomAffine(50),\n",
    "    v2.Resize((img_size, img_size)), \n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "\n",
    "cutmix = v2.CutMix(num_classes=len(train_set.classes))\n",
    "mixup = v2.MixUp(num_classes=len(train_set.classes))\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup], [0.5, 0.5])\n",
    "\n",
    "transform4 = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((img_size, img_size)), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    cutmix_or_mixup\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomRotation(50),\n",
    "    v2.Resize((img_size, img_size)), \n",
    "    v2.RandomErasing(0.5, (0.1,0.1)),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código utilizado para ler modelos salvos no disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Loading ######\n",
    "# recreate the model and optimizer\n",
    "model = Conv(len(train_set.classes))\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "#load the dict\n",
    "retrieved = torch.load(f'rrotateEraseDynamic.pt')\n",
    "\n",
    "# set the model and optimizer state to the saved state\n",
    "model.load_state_dict(retrieved['model'])\n",
    "model.to(device)\n",
    "optimizer.load_state_dict(retrieved['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000; Loss: 0.006688; Accuracy: 94.9891; Val Loss: 0.006714; Val Acc: 94.9315; Elapsed time: 56.7357\n",
      "Epoch: 001; Loss: 0.006684; Accuracy: 94.9646; Val Loss: 0.006720; Val Acc: 94.9027; Elapsed time: 49.8142\n",
      "Epoch: 002; Loss: 0.006670; Accuracy: 95.0251; Val Loss: 0.006560; Val Acc: 95.0006; Elapsed time: 49.8561\n",
      "Epoch: 003; Loss: 0.006693; Accuracy: 94.9790; Val Loss: 0.006623; Val Acc: 95.0524; Elapsed time: 48.5514\n",
      "Epoch: 004; Loss: 0.006685; Accuracy: 94.9473; Val Loss: 0.006664; Val Acc: 94.9603; Elapsed time: 49.6429\n",
      "Epoch: 005; Loss: 0.006691; Accuracy: 94.9675; Val Loss: 0.006537; Val Acc: 95.0582; Elapsed time: 49.6584\n",
      "Epoch: 006; Loss: 0.006656; Accuracy: 94.9833; Val Loss: 0.006649; Val Acc: 95.0755; Elapsed time: 50.3485\n",
      "Epoch: 007; Loss: 0.006632; Accuracy: 95.0639; Val Loss: 0.006618; Val Acc: 95.0524; Elapsed time: 50.1185\n",
      "Epoch: 008; Loss: 0.006664; Accuracy: 94.9574; Val Loss: 0.006642; Val Acc: 95.0236; Elapsed time: 50.1283\n",
      "Epoch: 009; Loss: 0.006636; Accuracy: 94.9963; Val Loss: 0.006559; Val Acc: 95.0639; Elapsed time: 50.3873\n",
      "Epoch: 010; Loss: 0.006629; Accuracy: 95.0380; Val Loss: 0.006586; Val Acc: 94.9948; Elapsed time: 50.3654\n",
      "Epoch: 011; Loss: 0.006673; Accuracy: 94.9372; Val Loss: 0.006546; Val Acc: 95.0524; Elapsed time: 49.9848\n",
      "Epoch: 012; Loss: 0.006625; Accuracy: 94.9703; Val Loss: 0.006851; Val Acc: 94.8566; Elapsed time: 50.1879\n",
      "Epoch: 013; Loss: 0.006680; Accuracy: 94.9905; Val Loss: 0.006581; Val Acc: 95.0467; Elapsed time: 50.4515\n",
      "Epoch: 014; Loss: 0.006637; Accuracy: 95.0006; Val Loss: 0.006625; Val Acc: 95.0294; Elapsed time: 50.7189\n",
      "Epoch: 015; Loss: 0.006648; Accuracy: 94.9876; Val Loss: 0.006732; Val Acc: 94.9315; Elapsed time: 50.1791\n",
      "Early stopping!\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "history = train_III(model, train_loader, val_loader, EPOCHS, loss_fn, optimizer, scheduler, early_stop, None, 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975771963596344"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente começamos por fazer a leitura dos modelos para serem utilizados no ensemble.\n",
    "\n",
    "Decidimos de todos os modelos apenas utilizar os melhores, e ignorar os que tem \\<90% de accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rrotateEraseDynamic.pt\n",
      "rrotateEraseBalanced.pt\n",
      "randomChannelPermutBalanced.pt\n",
      "horveraffineperspectiveNovo.pt\n",
      "cutmixMixup57.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m---> 17\u001b[0m acc\u001b[38;5;241m.\u001b[39mappend(\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 106\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    104\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m    107\u001b[0m      \n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# forward pass, compute the output of the model for the current batch\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# \"max\" returns a namedtuple (values, indices) where values is the maximum \u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# value of each row of the input tensor in the given dimension dim; \u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# indices is the index location of each maximum value found (argmax).\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# the argmax effectively provides the predicted class number        \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torchvision/datasets/folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torchvision/transforms/v2/_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(inpt, params) \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torchvision/transforms/v2/_type_conversion.py:39\u001b[0m, in \u001b[0;36mToImage._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m, inpt: Union[torch\u001b[38;5;241m.\u001b[39mTensor, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage, np\u001b[38;5;241m.\u001b[39mndarray], params: Dict[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tv_tensors\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torchvision/transforms/v2/functional/_type_conversion.py:16\u001b[0m, in \u001b[0;36mto_image\u001b[0;34m(inpt)\u001b[0m\n\u001b[1;32m     14\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(inpt)\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inpt, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m---> 16\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpil_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inpt, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m inpt\n",
      "File \u001b[0;32m~/miniconda3/envs/VCPI/lib/python3.8/site-packages/torchvision/transforms/functional.py:208\u001b[0m, in \u001b[0;36mpil_to_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(nppic)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    209\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "models = []\n",
    "acc = []\n",
    "\n",
    "i = 0\n",
    "for f in os.listdir():\n",
    "    \n",
    "    # if \".pt\" in f and f not in [\"rotateErase_30.pt\", \"rrotateErase_49.pt\", \"cutmixMixup.pt\", \"cutmixMixup43.pt\"]:\n",
    "    if \".pt\" in f and \"Balanced\" in f:\n",
    "        print(f)\n",
    "\n",
    "        reload = torch.load(f)\n",
    "        model = Conv(len(test_set.classes))\n",
    "        model.load_state_dict(reload['model'])\n",
    "        model.to(device)\n",
    "        models.append(model)\n",
    "        acc.append(evaluate(models[i], test_loader))\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de barras que mostra a avaliação de cada um dos modelos\n",
    "\n",
    "Podemos verificar que nem todos os modelos utilizados tem as melhores percentagens\n",
    "\n",
    "Isso é devido ao facto de apenas conseguirmos treinar um dos modelos com o dataset balanceado (randomChannelPermutBalanced), e ao facto de algumas das transformacoes utilizadas não serem ótimas para treinar a rede, que seria o caso da combinação de randomRotate com RandomErase, que, talvez por cortar parte da imagem, com o treino que lhe demos não conseguiu extrair das imagens de input o melhor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9758511185646057, 0.9780681133270264, 0.9060965776443481, 0.9765637516975403, 0.9570863246917725, 0.9140934348106384, 0.9774346947669983]\n",
      "0.9550277165004185 0.9780681133270264 0.9060965776443481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 1.0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(acc)\n",
    "index = [i for i in range(len(models))]\n",
    "\n",
    "plt.bar(index, acc)\n",
    "print(np.mean(acc), np.max(acc), np.min(acc))\n",
    "plt.ylim(0.80,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_labels_logits_and_preds(models):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = [[] for _ in range(len(models))]\n",
    "        labels = []\n",
    "\n",
    "        for images, labs in test_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels.extend(labs)\n",
    "            \n",
    "            for i in range(len(models)):\n",
    "                logits[i].extend(models[i](images).cpu())\n",
    "\n",
    "\n",
    "    return labels, logits\n",
    "\n",
    "labels, logits = get_labels_logits_and_preds(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [[] for _ in range(len(labels)) ]\n",
    "for index in range(len(labels)):\n",
    "    preds[index] = [np.argmax(logits[m][index].cpu().numpy()) for m in range(len(models))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_sum_of_logits(logits):\n",
    "\n",
    "    sum_logits = []\n",
    "\n",
    "    for i in range(len(logits[0])):\n",
    "\n",
    "        log = logits[0][i]\n",
    "        for m in range(len(models)):\n",
    "            log = np.add(log, logits[m][i])\n",
    "        sum_logits.append(np.argmax(log))\n",
    "    return(sum_logits)\n",
    "    \n",
    "class_logits = get_class_from_sum_of_logits(logits)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos resultados do modelo de ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olhando para os resultados abaixo, podemos ver que, com modelos cuja a melhor classificação era 97.8%, o ensemble desses modelos conseguiu uma percentagem de acerto de 98.6%, o que é um aumento ainda considerável.\n",
    "\n",
    "Podemos verificar que há casos em que todos acertam, que é a maioria, o que seria de esperar utilizando modelos com accuracy's acima dos 90%.\n",
    "\n",
    "Temos 1 caso no qual erram todos, 1592 casos com maioria a acertar, 73 empates e  103 com maioria errada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  12630\n",
      "All correct:  10861\n",
      "All incorrect:  1\n",
      "Majority correct:  1592\n",
      "Tie Vote:  73\n",
      "Majority Wrong:  103\n",
      "Percentage right:  0.9859857482185274\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "\n",
    "def get_stats(labels, class_preds, class_logits):\n",
    "\n",
    "    all_correct = 0\n",
    "    all_incorrect = 0\n",
    "    maj_vote = 0\n",
    "    maj_wrong = 0\n",
    "    tie = 0\n",
    "    count = 0\n",
    "\n",
    "    for k in range(len(labels)):\n",
    "\n",
    "        counter = collections.Counter(class_preds[k])\n",
    "        if len(counter) == 1:\n",
    "            if counter.most_common(1)[0][0] == labels[k]:\n",
    "                all_correct += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "        else:\n",
    "            aux = counter.most_common(2)\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] == labels[k]:\n",
    "                maj_vote += 1\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] != labels[k]:\n",
    "                maj_wrong += 1\n",
    "            elif aux[0][1] == aux[1][1]:\n",
    "                tie += 1\n",
    "\n",
    "        count += 1 \n",
    "        \n",
    "    return [count, all_correct, all_incorrect, maj_vote, tie, maj_wrong]\n",
    "    \n",
    "    \n",
    "res = get_stats(labels, preds, class_logits)\n",
    "print('total: ', res[0])\n",
    "print('All correct: ', res[1])\n",
    "print('All incorrect: ', res[2])\n",
    "print('Majority correct: ', res[3])\n",
    "print('Tie Vote: ', res[4])\n",
    "print('Majority Wrong: ', res[5])\n",
    "print('Percentage right: ', (res[1]+res[3])/res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "Estamos satisfeitos com os resultados obtidos, uma vez que conseguimos uma taxa de acerto de 98.6%, no entanto sabemos que, se tivéssemos conseguido treinar mais modelos com o dataset já balanceado, conseguiríamos provavelmente aumentar mais esse valor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VCPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
